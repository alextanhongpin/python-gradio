{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bcca1b4-0289-479e-a76f-36034f3d4d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "hf_api_key = os.environ['HF_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07144d2b-4740-4b6d-91cc-9d7edad15c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<InferenceClient(model='', timeout=None)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "client = InferenceClient(token=hf_api_key)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c8b393ee-a26e-47bb-905d-5b0120f4f573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Eiffel Tower is 324 metres (1,063 ft) tall and the tallest structure in Paris. It is the second tallest free-standing structure in France after the Millau Viaduct. It was the first structure to reach a height of 300 metres and it is now taller than the Chrysler Building by 5.2 metres.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ('''The tower is 324 metres (1,063 ft) tall, about the same height\n",
    "        as an 81-storey building, and the tallest structure in Paris. \n",
    "        Its base is square, measuring 125 metres (410 ft) on each side. \n",
    "        During its construction, the Eiffel Tower surpassed the Washington \n",
    "        Monument to become the tallest man-made structure in the world,\n",
    "        a title it held for 41 years until the Chrysler Building\n",
    "        in New York City was finished in 1930. It was the first structure \n",
    "        to reach a height of 300 metres. Due to the addition of a broadcasting \n",
    "        aerial at the top of the tower in 1957, it is now taller than the \n",
    "        Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the \n",
    "        Eiffel Tower is the second tallest free-standing structure in France \n",
    "        after the Millau Viaduct.''')\n",
    "\n",
    "client.summarization(text, model='philschmid/bart-large-cnn-samsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0587225-de45-40cc-858e-f69a8be231b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "63b8957f-e602-4845-94b7-42f5d218fbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 5001\n",
      "Closing server running on port: 5001\n",
      "Closing server running on port: 5001\n",
      "Closing server running on port: 5001\n",
      "Running on local URL:  http://127.0.0.1:5001\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:5001/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def summarize(input):\n",
    "    output = f\"Hello {input}\"# client.summarization(text, model='philschmid/bart-large-cnn-samsum')\n",
    "    return output\n",
    "\n",
    "gr.close_all()\n",
    "\n",
    "demo = gr.Interface(fn=summarize, \n",
    "                    inputs='text', \n",
    "                    outputs='text',)\n",
    "demo.launch(share=False, server_port=5001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d833201-a090-4306-8987-7d888fff6446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 5001\n",
      "Closing server running on port: 5001\n",
      "Closing server running on port: 5001\n",
      "Closing server running on port: 5001\n",
      "Closing server running on port: 5001\n"
     ]
    }
   ],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b48703f5-fcd1-4c77-b64c-d6f646af2ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:5001\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:5001/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.Interface(fn=summarize,\n",
    "                    inputs=[gr.Textbox(label='Text to summarize', lines=6)],\n",
    "                    outputs=[gr.Textbox(label='Result', lines=3)],\n",
    "                    title='Text Summarization',\n",
    "                    description='Summarize any text')\n",
    "demo.launch(share=False, server_port=5001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "888b423e-ebcb-4ae5-b26e-69b3e17797e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 5001\n",
      "Closing server running on port: 5001\n",
      "Closing server running on port: 5001\n",
      "Closing server running on port: 5001\n",
      "Closing server running on port: 5001\n",
      "Closing server running on port: 5001\n"
     ]
    }
   ],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e42c1743-b451-45bb-aefd-5d9cce61ea96",
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "502 Server Error: Bad Gateway for url: https://api-inference.huggingface.co/models/flair/ner-english-ontonotes-large",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/python/python-gradio/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:270\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/python/python-gradio/.venv/lib/python3.11/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 502 Server Error: Bad Gateway for url: https://api-inference.huggingface.co/models/flair/ner-english-ontonotes-large",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_classification\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHi, I am John\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mflair/ner-english-ontonotes-large\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/python/python-gradio/.venv/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:1696\u001b[0m, in \u001b[0;36mInferenceClient.token_classification\u001b[0;34m(self, text, model)\u001b[0m\n\u001b[1;32m   1657\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1658\u001b[0m \u001b[38;5;124;03mPerform token classification on the given text.\u001b[39;00m\n\u001b[1;32m   1659\u001b[0m \u001b[38;5;124;03mUsually used for sentence parsing, either grammatical, or Named Entity Recognition (NER) to understand keywords contained within text.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1693\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m payload: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: text}\n\u001b[0;32m-> 1696\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken-classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1700\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1701\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bytes_to_list(response)\n",
      "File \u001b[0;32m~/Documents/python/python-gradio/.venv/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:240\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 240\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m~/Documents/python/python-gradio/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:330\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequestError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 330\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 502 Server Error: Bad Gateway for url: https://api-inference.huggingface.co/models/flair/ner-english-ontonotes-large"
     ]
    }
   ],
   "source": [
    "client.token_classification('Hi, I am John', model='flair/ner-english-ontonotes-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "82b87184-7060-42b8-966c-78f537f83a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(input):\n",
    "    # output = client.token_classification('Hi, I am John', model='flair/ner-english-ontonotes-large')\n",
    "    output = [{'entity_group': 'PER',\n",
    "'score': 0.9971321225166321,\n",
    "'word': 'Sarah Jessica Parker',\n",
    "'start': 11,\n",
    "'end': 31},\n",
    "{'entity_group': 'PER',\n",
    "'score': 0.9773476123809814,\n",
    "'word': 'Jessica',\n",
    "'start': 52,\n",
    "'end': 59}]\n",
    "    return {'text': input, 'entities': output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "54c2635a-5d6a-4fe2-afcc-150d388e301f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:5001\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:5001/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.Interface(fn=ner,\n",
    "                    inputs=[gr.Textbox(label='Text to find entities', lines=2)],\n",
    "                    outputs=[gr.HighlightedText(label='Text with entities')],\n",
    "                    title='NER',\n",
    "                    description='Find entities',\n",
    "                    allow_flagging='never',\n",
    "                    examples=['Hi, my name is John', 'Apple is a trillion-dollar company'])\n",
    "demo.launch(share=False, server_port=5001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "66f365be-5cb8-4620-9502-b12a526e25df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 5001\n",
      "Closing server running on port: 5001\n",
      "Closing server running on port: 5001\n",
      "Closing server running on port: 5001\n",
      "Closing server running on port: 5001\n",
      "Closing server running on port: 5001\n",
      "Closing server running on port: 5001\n",
      "Closing server running on port: 5001\n"
     ]
    }
   ],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67ee472-6766-40c4-a200-17e0e2cad468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
